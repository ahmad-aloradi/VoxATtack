{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import sys\n",
    "from pathlib import Path\n",
    "from typing import Dict, Tuple, Literal\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from speechbrain.utils.metric_stats import EER\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "sys.path.append('/home/' + os.getenv('USER') + '/adversarial-robustness-for-sr')\n",
    "\n",
    "from src.modules.metrics.metrics import VerificationMetrics\n",
    "from src.modules.metrics.metrics import AS_norm "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scores Manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "MODELS_PATH = Path(\"/dataHDD/ahmad/hpc_results/second_run\")\n",
    "EVAL_MODE = \"SINGLE\"  # Options: \"SINGLE\", \"EVAL_ALL\"\n",
    "SINGLE_EXPERIMENT = \"vpc_amm_cyclic-available-max_dur10-32\"\n",
    "EXPERIMENT_PATTERN = \"vpc_amm_cyclic-*avail*-max_dur10-*32\"  # Default pattern\n",
    "EVAL_TEST = False  # Whether to evaluate test or validation data\n",
    "\n",
    "# Function to get experiments to evaluate\n",
    "def get_experiments_to_evaluate(mode=EVAL_MODE, \n",
    "                               single_exp=SINGLE_EXPERIMENT,\n",
    "                               pattern=EXPERIMENT_PATTERN):\n",
    "    \"\"\"Get list of experiments to evaluate based on mode\"\"\"\n",
    "    if mode == \"SINGLE\":\n",
    "        return [single_exp]\n",
    "    elif mode == \"EVAL_ALL\":\n",
    "        # Find all directories matching the pattern\n",
    "        matched_dirs = glob.glob(str(MODELS_PATH / pattern))\n",
    "        # Extract just the experiment names\n",
    "        experiments = [Path(d).name for d in matched_dirs]\n",
    "        print(f\"Found {len(experiments)} matching experiments\")\n",
    "        return experiments\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown EVAL_MODE: {mode}\")\n",
    "\n",
    "\n",
    "def get_df_path(experiment_name, eval_test, model_path_dir=MODELS_PATH):\n",
    "    model_exp = model_path_dir / experiment_name\n",
    "    \n",
    "    # Define file paths based on whether we're evaluating test or validation data\n",
    "    scores_csv_file = 'test_scores.csv' if eval_test else 'valid_best_scores.csv'\n",
    "    dirname = 'test' if eval_test else 'valid'\n",
    "    \n",
    "    # Find scores file\n",
    "    tmp = list(model_exp.rglob(f'{dirname}*/{scores_csv_file}'))\n",
    "    if not tmp:\n",
    "        raise ValueError(f\"⚠️ No {scores_csv_file} found for {experiment_name}\")\n",
    "  \n",
    "    assert len(tmp) == 1, f'Expected one file called {scores_csv_file}, found: {len(tmp)}'\n",
    "    return tmp[0]\n",
    "\n",
    "\n",
    "def process_experiment(experiment_name, eval_test=EVAL_TEST, model_path_dir=MODELS_PATH):\n",
    "    \"\"\"Process a single experiment and return the processed dataframe\"\"\"\n",
    "    print(f\"\\nProcessing experiment: {experiment_name}\")\n",
    "\n",
    "    # Load scores df\n",
    "    scores_path = get_df_path(experiment_name, eval_test, model_path_dir=MODELS_PATH)\n",
    "    df = pd.read_csv(f'{scores_path}')\n",
    "\n",
    "    # Load data df\n",
    "    data_df_file = 'test.csv' if eval_test else 'dev.csv'\n",
    "    df_test = pd.read_csv(f'{str(model_path_dir / experiment_name / f\"vpc2025_artifacts/{data_df_file}\")}', sep=\"|\")\n",
    "    \n",
    "    # Process dataframes\n",
    "    df['rel_filepath'] = df['audio_path'].apply(lambda x: x.split('vpc2025_official/')[-1])\n",
    "    df = df.merge(df_test[['speaker_id', 'rel_filepath', 'gender', 'recording_duration', 'text']], \n",
    "                    on='rel_filepath', how='left')\n",
    "    \n",
    "    print(f\"✓ Successfully processed data for {experiment_name}\")\n",
    "    return df\n",
    "\n",
    "\n",
    "# Sort experiments by the anonymization model name\n",
    "def extract_anon_model(experiment_name):\n",
    "    \"\"\"Extract the anonymization model name from the experiment pattern\"\"\"\n",
    "    parts = experiment_name.split('-')\n",
    "    if len(parts) < 3:\n",
    "        return experiment_name  # Return original if pattern doesn't match\n",
    "    return parts[1]  # The anonymization model is the second part\n",
    "\n",
    "\n",
    "# Get experiments based on current mode\n",
    "experiments = get_experiments_to_evaluate()\n",
    "experiments = sorted(experiments, key=extract_anon_model)\n",
    "print(f\"Will evaluate: {experiments}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_valid_embeddings(model_exp: str, eval_test: bool, device='cuda') -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    assert eval_test is False, \"This function is not implemented for test set\"\n",
    "\n",
    "    VALID_BEST_ENROL_EMBEDS = \"valid_artifacts/valid_best_enrol_embeds.pt\"\n",
    "    VALID_BEST_EMBEDS = \"valid_artifacts/valid_best_embeds.pt\"\n",
    "    \n",
    "    # Path to test cohort embeddings \n",
    "    test_artifacts_dir = os.path.join(model_exp, \"test_artifacts*\")\n",
    "    \n",
    "    # Find the most recent test artifacts directory\n",
    "    test_dirs = glob.glob(test_artifacts_dir)\n",
    "    if not test_dirs:\n",
    "        print(\"WARNING: No test cohort embeddings found\")\n",
    "        return None, None, None\n",
    "        \n",
    "    latest_test_dir = max(test_dirs, key=os.path.getctime)\n",
    "    \n",
    "    # Load validation embeddings\n",
    "    valid_enrol_embeds = torch.load(os.path.join(model_exp, f'{VALID_BEST_ENROL_EMBEDS}'), map_location=torch.device(device))\n",
    "    valid_embeds = torch.load(os.path.join(model_exp, f'{VALID_BEST_EMBEDS}'), map_location=torch.device(device))\n",
    "    cohort_embeds = torch.load(os.path.join(latest_test_dir, \"test_cohort_embeds.pt\"),  map_location=torch.device(device))\n",
    "\n",
    "    print(f\"✓ Loaded embeddings for {model_exp}\")\n",
    "    return valid_enrol_embeds, valid_embeds, cohort_embeds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_radar_plot(eer_results_speaker,\n",
    "                      output_path,\n",
    "                      title=\"Speaker-specific EER\",\n",
    "                      description=\"Equal Error Rate (EER) across different speakers\"):\n",
    "    fig = plt.figure(figsize=(12, 8))\n",
    "    ax = fig.add_axes([0.1, 0.1, 0.6, 0.8], polar=True)\n",
    "    \n",
    "    labels = list(eer_results_speaker.keys())\n",
    "    values = list(eer_results_speaker.values())\n",
    "    labels = list(eer_results_speaker.keys())\n",
    "    values = list(eer_results_speaker.values())\n",
    "    num_vars = len(labels)\n",
    "    angles = np.linspace(0, 2 * np.pi, num_vars, endpoint=False).tolist()\n",
    "    \n",
    "    values += values[:1]\n",
    "    angles += angles[:1]\n",
    "    \n",
    "    ax.plot(angles, values, color='#FF6B6B', linewidth=2, marker='o', \n",
    "           markersize=8, label='EER Values')\n",
    "    \n",
    "    ax.grid(color='gray', alpha=0.2, linestyle='--', linewidth=1)\n",
    "    ax.set_ylim(0, max(values) * 1.2)\n",
    "    \n",
    "    ax.set_xticks(angles[:-1])\n",
    "    ax.set_xticklabels(labels, fontsize=10, fontweight='bold')\n",
    "    \n",
    "    for i in range(num_vars):\n",
    "        angle_rad = angles[i]\n",
    "        angle_deg = angle_rad * 180 / np.pi\n",
    "        \n",
    "        if angle_deg > 90 and angle_deg < 270:\n",
    "            rotation = angle_deg + 180\n",
    "        else:\n",
    "            rotation = angle_deg\n",
    "            \n",
    "        ax.text(angle_rad, values[i] + max(values) * 0.1,\n",
    "                f'{values[i]:.2f}',\n",
    "                ha='center', va='center',\n",
    "                rotation=rotation,\n",
    "                fontsize=9,\n",
    "                bbox=dict(facecolor='white', \n",
    "                         edgecolor='none',\n",
    "                         alpha=0.8,\n",
    "                         pad=2))\n",
    "    \n",
    "    plt.title(title, pad=20, fontsize=14, fontweight='bold')\n",
    "    ax.legend(loc='center left', bbox_to_anchor=(1.1, 0.5))\n",
    "\n",
    "    plt.savefig(output_path, \n",
    "                dpi=300, \n",
    "                bbox_inches='tight',\n",
    "                pad_inches=0.5,\n",
    "                format='png',\n",
    "                transparent=True)\n",
    "    plt.close()\n",
    "    \n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute gender/speaker metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gender_metrics(df: pd.DataFrame, scores_col: Literal['score', 'norm_score'] = 'score') -> Tuple[Dict, Dict, Dict]:\n",
    "    \"\"\"\n",
    "    Compute verification metrics for each gender.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with columns ['gender', score', 'label']\n",
    "        scores_col: analyze raw scores or normalzied scores (score or norm_score) \n",
    "    \n",
    "    Returns:\n",
    "        gender_metrics: Dict of metrics per gender\n",
    "        gender_curves: Dict of curve data per gender\n",
    "        gender_eer: Dict of EER values per gender\n",
    "    \"\"\"\n",
    "    gender_metrics = {}\n",
    "    gender_curves = {}\n",
    "    gender_eer = {}\n",
    "    metric = VerificationMetrics()\n",
    "    \n",
    "    for gender in df['gender'].unique():\n",
    "        metric.reset()\n",
    "        gender_data = df[df['gender'] == gender]\n",
    "        \n",
    "        # Compute metrics\n",
    "        metric.update(\n",
    "            torch.from_numpy(gender_data[scores_col].values),\n",
    "            torch.from_numpy(gender_data['label'].values)\n",
    "        )\n",
    "        metrics = metric.compute()\n",
    "        \n",
    "        # Store results\n",
    "        gender_metrics[gender] = metrics\n",
    "        gender_curves[gender] = {\n",
    "            k: v.detach().cpu().numpy() \n",
    "            for k, v in metric._curve_data.items()\n",
    "        }\n",
    "        gender_eer[gender] = metrics['eer']\n",
    "        \n",
    "    return gender_metrics, gender_curves, gender_eer\n",
    "\n",
    "\n",
    "def plot_gender_det_curves(gender_curves: Dict, gender_metrics: Dict, eps: float = 1e-8) -> plt.Figure:\n",
    "    \"\"\"\n",
    "    Plot DET curves for multiple speakers.\n",
    "    \n",
    "    Args:\n",
    "        gender_curves: Dict of curve data per speaker\n",
    "        gender_metrics: Dict of metrics per speaker\n",
    "        eps: Small value for numerical stability\n",
    "    \n",
    "    Returns:\n",
    "        Matplotlib figure\n",
    "    \"\"\"\n",
    "    plt.style.use('seaborn-v0_8-paper')\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    \n",
    "    # Get sorted genders for consistent ordering\n",
    "    genders = sorted(gender_curves.keys())\n",
    "    \n",
    "    # Define fixed colors for consistency\n",
    "    GENDER_COLORS = {'female': '#FF69B4','male': '#4169E1'}\n",
    "    \n",
    "    # Plot each speaker's curve\n",
    "    mean_eer = []\n",
    "    for gender in genders:\n",
    "        curves = gender_curves[gender]\n",
    "        metrics = gender_metrics[gender]\n",
    "        \n",
    "        far = np.maximum(curves['far'], eps)\n",
    "        frr = np.maximum(curves['frr'], eps)\n",
    "        eer = metrics['eer']\n",
    "        mean_eer.append(eer)\n",
    "        \n",
    "        ax.plot(far, frr, '-', color=GENDER_COLORS.get(gender, 'gray'), alpha=0.5, linewidth=1, label=f'{gender} (EER: {eer:.5f})')\n",
    "    \n",
    "    # Plot mean EER point for visual clarity\n",
    "    mean_eer_value = np.mean(mean_eer)\n",
    "    ax.plot(mean_eer_value, mean_eer_value, 'ko', markersize=8, label=f'Mean EER: {mean_eer_value:.4f}')\n",
    "    \n",
    "    # Plot diagonal\n",
    "    ax.plot([eps, 1], [eps, 1], 'k--', alpha=0.3, linewidth=1)\n",
    "    \n",
    "    # Customize plot\n",
    "    ax.set_xscale('log')\n",
    "    ax.set_yscale('log')\n",
    "    ax.set_xlabel('False Acceptance Rate (FAR)')\n",
    "    ax.set_ylabel('False Rejection Rate (FRR)')\n",
    "    ax.set_title('Detection Error Tradeoff (DET) Curves by Gender')\n",
    "    ax.grid(True, which='both', linestyle='--', alpha=0.3)\n",
    "    ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left', frameon=True, fancybox=False, edgecolor='black')\n",
    "    \n",
    "    fig.tight_layout()\n",
    "    return fig\n",
    "\n",
    "\n",
    "def compute_speaker_metrics(df: pd.DataFrame) -> Tuple[Dict, Dict, Dict]:\n",
    "    \"\"\"\n",
    "    Compute verification metrics for each speaker.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with columns ['enrollment_id', 'speaker_id', 'score', 'label']\n",
    "    \n",
    "    Returns:\n",
    "        speaker_metrics: Dict of metrics per speaker\n",
    "        speaker_curves: Dict of curve data per speaker\n",
    "        speaker_eer: Dict of EER values per speaker ID\n",
    "    \"\"\"\n",
    "    speaker_metrics = {}\n",
    "    speaker_curves = {}\n",
    "    speaker_eer = {}\n",
    "    metric = VerificationMetrics()\n",
    "    \n",
    "    for speaker in df['enrollment_id'].unique():\n",
    "        metric.reset()\n",
    "        speaker_data = df[df['speaker_id'] == speaker]\n",
    "        \n",
    "        # Compute metrics\n",
    "        metric.update(\n",
    "            torch.from_numpy(speaker_data['score'].values),\n",
    "            torch.from_numpy(speaker_data['label'].values)\n",
    "        )\n",
    "        metrics = metric.compute()\n",
    "        \n",
    "        # Store results\n",
    "        speaker_metrics[speaker] = metrics\n",
    "        speaker_curves[speaker] = {\n",
    "            k: v.detach().cpu().numpy() \n",
    "            for k, v in metric._curve_data.items()\n",
    "        }\n",
    "        speaker_eer[speaker.split('_')[-1]] = metrics['eer']\n",
    "        \n",
    "    return speaker_metrics, speaker_curves, speaker_eer\n",
    "\n",
    "\n",
    "def plot_speaker_det_curves(speaker_curves: Dict, \n",
    "                            speaker_metrics: Dict,\n",
    "                            eps: float = 1e-8,\n",
    "                            max_speakers: int = None) -> plt.Figure:\n",
    "    \"\"\"\n",
    "    Plot DET curves for multiple speakers.\n",
    "    \n",
    "    Args:\n",
    "        speaker_curves: Dict of curve data per speaker\n",
    "        speaker_metrics: Dict of metrics per speaker\n",
    "        eps: Small value for numerical stability\n",
    "        max_speakers: Maximum number of speakers to plot (None for all)\n",
    "    \n",
    "    Returns:\n",
    "        Matplotlib figure\n",
    "    \"\"\"\n",
    "    plt.style.use('seaborn-v0_8-paper')\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    \n",
    "    # Select speakers to plot\n",
    "    speakers = sorted(speaker_curves.keys())\n",
    "    if max_speakers:\n",
    "        speakers = speakers[:max_speakers]\n",
    "    \n",
    "    # Get color map for speakers\n",
    "    colors = plt.cm.tab20(np.linspace(0, 1, len(speakers)))\n",
    "    \n",
    "    # Plot each speaker's curve\n",
    "    mean_eer = []\n",
    "    for speaker, color in zip(speakers, colors):\n",
    "        curves = speaker_curves[speaker]\n",
    "        metrics = speaker_metrics[speaker]\n",
    "        \n",
    "        far = np.maximum(curves['far'], eps)\n",
    "        frr = np.maximum(curves['frr'], eps)\n",
    "        eer = metrics['eer']\n",
    "        mean_eer.append(eer)\n",
    "        \n",
    "        ax.plot(far, frr, '-', color=color, alpha=0.5, linewidth=1,\n",
    "               label=f'Speaker {speaker.split(\"_\")[-1]} (EER: {eer:.5f})')\n",
    "    \n",
    "    # Plot mean EER point for visual clarity\n",
    "    mean_eer_value = np.mean(mean_eer)\n",
    "    ax.plot(mean_eer_value, mean_eer_value, 'ko', markersize=8,\n",
    "           label=f'Mean EER: {mean_eer_value:.5f}')\n",
    "    \n",
    "    # Plot diagonal\n",
    "    ax.plot([eps, 1], [eps, 1], 'k--', alpha=0.3, linewidth=1)\n",
    "    \n",
    "    # Customize plot\n",
    "    ax.set_xscale('log')\n",
    "    ax.set_yscale('log')\n",
    "    ax.set_xlabel('False Acceptance Rate (FAR)')\n",
    "    ax.set_ylabel('False Rejection Rate (FRR)')\n",
    "    ax.set_title('Detection Error Tradeoff (DET) Curves by Speaker')\n",
    "    ax.grid(True, which='both', linestyle='--', alpha=0.3)\n",
    "    ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left', \n",
    "             frameon=True, fancybox=False, edgecolor='black')\n",
    "    \n",
    "    fig.tight_layout()\n",
    "    return fig\n",
    "\n",
    "def analyze_speaker_results(df: pd.DataFrame, output_path: str, max_speakers: int = None):\n",
    "    \"\"\"\n",
    "    Analyze and plot verification results from scores file.\n",
    "    \n",
    "    Args:\n",
    "        df: CSV dataframe\n",
    "        output_path: path to save the figure\n",
    "        max_speakers: Maximum number of speakers to plot\n",
    "    \"\"\"\n",
    "    # Compute metrics\n",
    "    speaker_metrics, speaker_curves, speaker_eer = compute_speaker_metrics(df)\n",
    "    \n",
    "    # Plot DET curves\n",
    "    det_fig = plot_speaker_det_curves(\n",
    "        speaker_curves, \n",
    "        speaker_metrics,\n",
    "        max_speakers=max_speakers\n",
    "    )\n",
    "    plt.savefig(output_path,\n",
    "                dpi=300,\n",
    "                bbox_inches='tight',\n",
    "                pad_inches=0.5,\n",
    "                format='png',\n",
    "                transparent=True)\n",
    "    plt.show()\n",
    "    \n",
    "    return det_fig, speaker_metrics, speaker_eer\n",
    "\n",
    "\n",
    "def analyze_gender_results(df: pd.DataFrame, output_path: str, scores_col: Literal['score', 'norm_score']):\n",
    "    \"\"\"\n",
    "    Analyze and plot verification results from scores file.\n",
    "    \n",
    "    Args:\n",
    "        df: CSV dataframe\n",
    "        output_path: path to save the figure\n",
    "        max_speakers: Maximum number of speakers to plot\n",
    "        scores_col: analyze raw scores or normalzied scores (score or norm_score)\n",
    "    \"\"\"\n",
    "    # Compute metrics\n",
    "    gender_metrics, gender_curves, gender_eer = compute_gender_metrics(df, scores_col=scores_col)\n",
    "    \n",
    "    # Plot DET curves\n",
    "    det_fig = plot_gender_det_curves(gender_curves, gender_metrics)\n",
    "    \n",
    "    plt.savefig(output_path,\n",
    "                dpi=300,\n",
    "                bbox_inches='tight',\n",
    "                pad_inches=0.5,\n",
    "                format='png',\n",
    "                transparent=True)\n",
    "    plt.show()\n",
    "    \n",
    "    return det_fig, gender_metrics, gender_eer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Trial Scores  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bin_count(data):\n",
    "    \"\"\"Calculate optimal number of bins (Freedman-Diaconis rule)\"\"\"\n",
    "    iqr = np.percentile(data, 75) - np.percentile(data, 25)\n",
    "    bin_width = 2 * iqr * len(data)**(-1/3)  # Freedman-Diaconis rule\n",
    "    return int(np.ceil((data.max() - data.min()) / bin_width)) if bin_width > 0 else 30\n",
    "\n",
    "\n",
    "def plot_score_distribution(df_test, score_schemes=None, figsize=(10, 6), save_dir=None, show_plot=True):\n",
    "    \"\"\"\n",
    "    Plot the distribution of similarity scores for genuine and impostor trials.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df_test : pandas.DataFrame\n",
    "        DataFrame containing similarity scores and labels\n",
    "    score_schemes : list, optional\n",
    "        List of score column names to plot (default: ['score', 'norm_score'] or available columns)\n",
    "    figsize : tuple, optional\n",
    "        Figure size as (width, height) in inches (default: (10, 6))\n",
    "    save_dir : str, optional\n",
    "        Directory path to save plots. If None, plots won't be saved.\n",
    "    show_plot : bool, optional\n",
    "        Whether to display the plot interactively (default: True)\n",
    "    \"\"\"\n",
    "    # Set publication-quality font settings\n",
    "    plt.rcParams.update({\n",
    "        'font.family': 'serif',\n",
    "        'font.serif': ['Times New Roman', 'DejaVu Serif', 'Palatino', 'Computer Modern Roman'],\n",
    "        'font.size': 12,\n",
    "        'axes.labelsize': 14,\n",
    "        'axes.titlesize': 16,\n",
    "        'xtick.labelsize': 12,\n",
    "        'ytick.labelsize': 12,\n",
    "        'legend.fontsize': 12,\n",
    "        'figure.titlesize': 18,\n",
    "        'text.usetex': False,  # Set to True if you have LaTeX installed\n",
    "        'axes.linewidth': 1.2,\n",
    "        'xtick.major.width': 1.2,\n",
    "        'ytick.major.width': 1.2,\n",
    "        'xtick.major.size': 5,\n",
    "        'ytick.major.size': 5\n",
    "    })\n",
    "    \n",
    "    # Set default score schemes if not provided\n",
    "    if score_schemes is None:\n",
    "        default_schemes = ['score', 'norm_score']\n",
    "        # Filter to only keep schemes that exist in the dataframe\n",
    "        score_schemes = [scheme for scheme in default_schemes if scheme in df_test.columns]\n",
    "        \n",
    "        # If none of the default schemes exist, try to find any columns that might be scores\n",
    "        if not score_schemes:\n",
    "            # Look for columns that might contain scores (excluding 'label')\n",
    "            potential_scores = [col for col in df_test.columns if col != 'label']\n",
    "            if potential_scores:\n",
    "                score_schemes = potential_scores[:2]  # Take at most 2 columns\n",
    "                print(f\"Using detected score columns: {score_schemes}\")\n",
    "            else:\n",
    "                raise ValueError(\"No score columns found in the dataframe.\")\n",
    "    else:\n",
    "        # Filter user-provided schemes to only keep those that exist in the dataframe\n",
    "        valid_schemes = [scheme for scheme in score_schemes if scheme in df_test.columns]\n",
    "        if not valid_schemes:\n",
    "            raise ValueError(f\"None of the provided score schemes {score_schemes} exist in the dataframe.\")\n",
    "        \n",
    "        if len(valid_schemes) < len(score_schemes):\n",
    "            print(f\"Warning: Only using valid columns: {valid_schemes}. Skipping non-existent columns.\")\n",
    "        \n",
    "        score_schemes = valid_schemes\n",
    "    \n",
    "    # Create save directory if it doesn't exist\n",
    "    if save_dir and not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    \n",
    "    for score_scheme in score_schemes:\n",
    "        # Set a professional style with whitegrid for clarity\n",
    "        sns.set_style('whitegrid', {\n",
    "            'grid.linestyle': '--',\n",
    "            'grid.alpha': 0.7,\n",
    "            'axes.edgecolor': '0.2',\n",
    "            'axes.grid': True\n",
    "        })\n",
    "        \n",
    "        # Create figure with higher resolution for publication\n",
    "        fig = plt.figure(figsize=figsize, dpi=300)\n",
    "        ax = fig.add_subplot(111)\n",
    "        \n",
    "        # Extract scores by label\n",
    "        neg_scores = df_test[df_test.label == 0][score_scheme]\n",
    "        pos_scores = df_test[df_test.label == 1][score_scheme]\n",
    "        \n",
    "        # Get count of trials\n",
    "        neg_count = len(neg_scores)\n",
    "        pos_count = len(pos_scores)\n",
    "        total_count = neg_count + pos_count\n",
    "        \n",
    "        # Calculate appropriate bin count with upper limit\n",
    "        bins = max(get_bin_count(neg_scores), get_bin_count(pos_scores))\n",
    "#         bins = min(bins, 100)  # Cap at 100 bins for visualization clarity\n",
    "        \n",
    "        # Calculate statistics once\n",
    "        neg_mean = neg_scores.mean()\n",
    "        pos_mean = pos_scores.mean()\n",
    "        separation = pos_mean - neg_mean\n",
    "        \n",
    "        # Create normalized histograms with more professional colors\n",
    "        ax.hist(neg_scores, bins=bins, alpha=0.7, density=True, \n",
    "                color='#D45E5E', edgecolor='#8B0000', linewidth=1.2, \n",
    "                label=f'Impostor Trials (n={neg_count:,})')\n",
    "        ax.hist(pos_scores, bins=bins, alpha=0.7, density=True,\n",
    "                color='#5E81D4', edgecolor='#00008B', linewidth=1.2,\n",
    "                label=f'Genuine Trials (n={pos_count:,})')\n",
    "        \n",
    "        # Add vertical lines for means with publication-quality styling\n",
    "        ax.axvline(neg_mean, color='#8B0000', linestyle='dashed', linewidth=2,\n",
    "                  label=f'Impostor Mean: {neg_mean:.5f}')\n",
    "        ax.axvline(pos_mean, color='#00008B', linestyle='dashed', linewidth=2,\n",
    "                  label=f'Genuine Mean: {pos_mean:.5f}')\n",
    "        \n",
    "        # Add labels and title with improved formatting\n",
    "        score_name = score_scheme.replace('_', ' ').title()\n",
    "        ax.set_xlabel('Similarity Score', fontweight='bold')\n",
    "        ax.set_ylabel('Normalized Frequency', fontweight='bold')\n",
    "        ax.set_title(f'Distribution of Similarity Scores by Trial Type ({score_name})', \n",
    "                    fontweight='bold', pad=15)\n",
    "        \n",
    "        # Customize legend with cleaner appearance\n",
    "        legend = ax.legend(frameon=True, fancybox=False, framealpha=0.95, \n",
    "                          edgecolor='0.2', loc='best')\n",
    "        legend.get_frame().set_linewidth(1.0)\n",
    "        \n",
    "        # Add annotation about the separation and trial counts with cleaner styling\n",
    "        info_text = f\"Separation: {separation:.5f}\\nTotal Trials: {total_count:,}\\nImpostor: {neg_count:,} ({neg_count/total_count:.1%})\\nGenuine: {pos_count:,} ({pos_count/total_count:.1%})\"\n",
    "\n",
    "        ax.annotate(info_text,\n",
    "                   xy=(0.97, 0.97), xycoords='axes fraction', \n",
    "                   bbox=dict(boxstyle=\"round,pad=0.4\", fc=\"#F8F8F8\", ec=\"gray\", \n",
    "                            alpha=0.95, linewidth=1.2),\n",
    "                   ha='right', va='top', fontweight='normal',  # Changed from 'bold' to 'normal'\n",
    "                   fontsize=plt.rcParams['legend.fontsize'])   # Match legend font size\n",
    "        \n",
    "        # Customize spines and ticks for publication quality\n",
    "        for spine in ['top', 'right', 'bottom', 'left']:\n",
    "            ax.spines[spine].set_linewidth(1.2)\n",
    "        ax.tick_params(width=1.2, length=5, direction='out')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save the plot if directory is provided\n",
    "        if save_dir:\n",
    "            # Create filename from score scheme with high resolution\n",
    "            filename = f\"{score_scheme.replace(' ', '_')}_distribution.pdf\"  # PDF for vector quality\n",
    "            filepath = os.path.join(save_dir, filename)\n",
    "            plt.savefig(filepath, dpi=600, bbox_inches='tight', format='pdf')\n",
    "            \n",
    "            # Also save a PNG version for easy viewing\n",
    "            png_filepath = os.path.join(save_dir, f\"{score_scheme.replace(' ', '_')}_distribution.png\")\n",
    "            plt.savefig(png_filepath, dpi=300, bbox_inches='tight')\n",
    "            \n",
    "            print(f\"Saved plot to {filepath} and {png_filepath}\")\n",
    "        \n",
    "        # Show the plot if requested\n",
    "        if show_plot:\n",
    "            plt.show()\n",
    "        else:\n",
    "            plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_dev(df, cohort_embeddings, trial_embeds, enrol_embeds, device, batch_size=8):\n",
    "    \"\"\"Validation step for trials.\"\"\"\n",
    "    # iterate over batches \n",
    "    results = []\n",
    "    metric = VerificationMetrics()\n",
    "    # df = df.rename({'label': 'trial_label', 'enrollment_id': 'enroll_id'}, axis=1)\n",
    "    df = df.rename({'enrollment_id': 'enroll_id'}, axis=1)\n",
    "    \n",
    "    for i in tqdm(range(0, len(df), batch_size)):\n",
    "        batch = df.iloc[i: i + batch_size]\n",
    "        batch_dict, normalized_scores = _trials_eval_step(batch, cohort_embeddings, trial_embeds, enrol_embeds, device='cuda')\n",
    "        metric.update(scores=normalized_scores, labels=torch.tensor(batch.label.tolist()))\n",
    "        results.append(batch_dict)\n",
    "    \n",
    "    metrics = metric.compute()\n",
    "\n",
    "    scores = pd.DataFrame([\n",
    "                {\"enrollment_id\": enroll_id,\n",
    "                \"audio_path\": audio_path,\n",
    "                \"label\": label,\n",
    "                \"score\": score,\n",
    "                \"norm_score\": norm_score,\n",
    "                \"model\": model,                \n",
    "                }\n",
    "                for batch in results\n",
    "                for enroll_id, audio_path, label, score, norm_score, model in zip(\n",
    "                    batch[\"enrollment_id\"],\n",
    "                    batch[\"audio_path\"],\n",
    "                    batch[\"label\"],\n",
    "                    batch[\"score\"],\n",
    "                    batch[\"norm_score\"],\n",
    "                    batch[\"model\"],\n",
    "                )\n",
    "            ])\n",
    "    \n",
    "    # Update scores DataFrame with computed metrics\n",
    "    scores.loc[:, metrics.keys()] = [v.item() if torch.is_tensor(v) else v for v in metrics.values()]\n",
    "    return scores\n",
    "    \n",
    "\n",
    "def _trials_eval_step(batch, cohort_embeddings, trial_embeds, enrol_embeds, device):\n",
    "    trial_embeddings = torch.stack([trial_embeds[path] for path in batch.audio_path]).to(device)\n",
    "    enroll_embeddings = torch.stack([enrol_embeds[model][enroll_id]\n",
    "                                for model, enroll_id in zip(batch.model, batch.enroll_id)]).to(device)\n",
    "\n",
    "    # Compute raw cosine similarity scores\n",
    "    raw_scores = torch.nn.functional.cosine_similarity(enroll_embeddings, trial_embeddings)\n",
    "    \n",
    "    if cohort_embeddings is not None:\n",
    "        normalized_scores = []\n",
    "        for i, (enroll_emb, test_emb, model) in enumerate(zip(enroll_embeddings, trial_embeddings, batch.model)):\n",
    "            \n",
    "            # Get model-specific cohort embeddings\n",
    "            model_cohort = cohort_embeddings.get(model)#.to(device)\n",
    "            assert model_cohort is not None, f\"No cohort embeddings found for model {model}\"\n",
    "            if isinstance(model_cohort, dict):\n",
    "                model_cohort = torch.stack(list(model_cohort.values()))\n",
    "            if model_cohort.ndim != 2:\n",
    "                raise ValueError(f\"Invalid cohort embeddings shape for model {model}: {model_cohort.shape}\")\n",
    "            \n",
    "            # Apply AS-Norm\n",
    "            norm_score = AS_norm(score=raw_scores[i],\n",
    "                                 enroll_embedding=enroll_emb,\n",
    "                                 test_embedding=test_emb, \n",
    "                                 cohort_embeddings=model_cohort,\n",
    "                                 **{'topk': 10000, 'min_cohort_size': 3000})\n",
    "            normalized_scores.append(norm_score)\n",
    "        \n",
    "        # Convert back to tensor\n",
    "        normalized_scores = torch.tensor(normalized_scores, device=raw_scores.device)\n",
    "    \n",
    "    else:\n",
    "        normalized_scores = raw_scores.clone()\n",
    "    \n",
    "    batch_dict = {\n",
    "        \"enrollment_id\": batch.enroll_id,\n",
    "        \"audio_path\": batch.audio_path,\n",
    "        \"label\": batch.label,\n",
    "        \"score\": raw_scores.detach().cpu().tolist(),\n",
    "        \"norm_score\": normalized_scores.detach().cpu().tolist(),\n",
    "        \"model\": batch.model,\n",
    "    }\n",
    "\n",
    "    return batch_dict, normalized_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_analysis_header(anon_model: str):\n",
    "    header = f\"\"\"\n",
    "    <div style=\"background-color: #f0f2f6; padding: 10px; border-radius: 5px; margin: 10px 0;\">\n",
    "        <h3 style=\"color: #2c3e50; margin: 0; text-align: center;\">\n",
    "            Analyzing Anonymization System: {anon_model}\n",
    "        </h3>\n",
    "    </div>\n",
    "    \"\"\"\n",
    "    display(HTML(header))\n",
    "\n",
    "def print_analysis_results(speaker_eer, gender_eer):\n",
    "    results = f\"\"\"\n",
    "    <div style=\"background-color: #f8f9fa; padding: 15px; border-radius: 5px; margin: 10px 0;\">\n",
    "        <h4 style=\"color: #2c3e50; margin: 0 0 10px 0;\">Results Summary:</h4>\n",
    "        <p style=\"margin: 5px 0; color: #34495e;\">\n",
    "            <b>Speaker Results:</b><br>\n",
    "            Mean EER: {np.mean(list(speaker_eer.values())):.4f} ± {np.std(list(speaker_eer.values())):.4f}\n",
    "        </p>\n",
    "        <p style=\"margin: 5px 0; color: #34495e;\">\n",
    "            <b>Gender Results:</b><br>\n",
    "            Mean EER: {np.mean(list(gender_eer.values())):.4f} ± {np.std(list(gender_eer.values())):.4f}\n",
    "        </p>\n",
    "    </div>\n",
    "    \"\"\"\n",
    "    display(HTML(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "results = {}\n",
    "\n",
    "for experiment in experiments:\n",
    "\n",
    "    df = process_experiment(experiment, eval_test=EVAL_TEST, model_path_dir=MODELS_PATH)\n",
    "\n",
    "    #################### Normalize scores for validation set ########################\n",
    "    if not EVAL_TEST and all(df.score == df.norm_score):\n",
    "        valid_enrol_embeds, valid_embeds, cohort_embeds = load_valid_embeddings(\n",
    "            model_exp=MODELS_PATH / experiment, eval_test=EVAL_TEST, device=device)\n",
    "        \n",
    "        # Perform normalization evaluation on the validation set\n",
    "        df = eval_dev(df, cohort_embeds, valid_embeds, valid_enrol_embeds, device=device)\n",
    "        scores_path = get_df_path(experiment, eval_test=EVAL_TEST, model_path_dir=MODELS_PATH)\n",
    "        df.to_csv(scores_path, index=False)\n",
    "\n",
    "        # merge with dev.csv for metdata\n",
    "        df_test = pd.read_csv(f'{str(MODELS_PATH / experiment / f\"vpc2025_artifacts/dev.csv\")}', sep=\"|\")\n",
    "        df['rel_filepath'] = df['audio_path'].apply(lambda x: x.split('vpc2025_official/')[-1])\n",
    "        df = df.merge(df_test[['speaker_id', 'rel_filepath', 'gender', 'recording_duration', 'text']], on='rel_filepath', how='left')\n",
    "        ##############################################################################\n",
    "        \n",
    "    results[experiment] = df\n",
    "    tot_speaker_metrics = {}\n",
    "    tot_gender_metrics = {}\n",
    "    data_split = 'test' if EVAL_TEST else 'dev'\n",
    "    results_dir = f'results/{experiment}/{data_split}'\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "     \n",
    "    for anon_model in df['model'].unique():\n",
    "        df_test = df[df['model'] == anon_model]\n",
    "        \n",
    "        print_analysis_header(anon_model)\n",
    "\n",
    "        # Plot scores histogram\n",
    "        plot_score_distribution(df_test, figsize=(12, 8), save_dir=f'{results_dir}/{anon_model}', show_plot=True)\n",
    "        \n",
    "        fig_speaker, speaker_metrics, speaker_eer = analyze_speaker_results(\n",
    "            df_test, output_path=f'{results_dir}/{anon_model}/speaker_DET_{anon_model}.png', max_speakers=None)\n",
    "\n",
    "        scores_cols = ['score'] if all(df.score == df.norm_score) else ['score', 'norm_score']\n",
    "        for scores_col in scores_cols:\n",
    "            norm = 'Normalized' if scores_col == 'norm_score' else 'Raw'\n",
    "            fig_gender, gender_metrics, gender_eer = analyze_gender_results(\n",
    "                df_test, output_path=f'{results_dir}/{anon_model}/gender_DET_{anon_model}_{norm}Scores.png', scores_col=scores_col)\n",
    "        \n",
    "        print_analysis_results(speaker_eer, gender_eer)\n",
    "        \n",
    "        # Save results as csvs\n",
    "        tot_speaker_metrics[anon_model] = pd.DataFrame.from_records(speaker_metrics).astype(float)\n",
    "        tot_gender_metrics[anon_model] = pd.DataFrame.from_records(gender_metrics).astype(float)\n",
    "        tot_speaker_metrics[anon_model].to_csv(f'{results_dir}/{anon_model}/per_speaker_results_{anon_model}.csv')\n",
    "        tot_gender_metrics[anon_model].to_csv(f'{results_dir}/{anon_model}/per_gender_results_{anon_model}.csv')\n",
    "\n",
    "        # Create radar plot\n",
    "        create_radar_plot(speaker_eer, output_path=f'{results_dir}/{anon_model}/speaker_eer_{anon_model}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare results if in EVAL_ALL mode\n",
    "if EVAL_MODE == \"EVAL_ALL\" and len(results) > 1:\n",
    "    print(\"\\n===== Comparison of Results =====\")\n",
    "    for exp, df in results.items():\n",
    "        # Example metrics for comparison\n",
    "        eer = df.get('eer', [0])[0] if 'eer' in df else \"N/A\"  # Adjust based on your actual metrics\n",
    "        print(f\"{exp}: EER={eer}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
