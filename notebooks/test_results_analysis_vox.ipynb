{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os \n",
    "import sys\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Literal\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import torch\n",
    "from speechbrain.utils.metric_stats import EER\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from copy import deepcopy\n",
    "\n",
    "sys.path.append('/home/ahmad/adversarial-robustness-for-sr')\n",
    "from src.modules.metrics.metrics import VerificationMetrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scores Manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_filename = \"vpc_amm_29March\" # vpc_amm_29March last_hidden_exp_comprehensive 2025-02-02_22-12-58, 2025-02-09_01-18-15 2025-02-12_14-13-10 2025-02-15_22-03-15 2025-02-21_17-50-04\n",
    "# MODELS_PATH = '/home.local/aloradi/logs/train/runs'\n",
    "# MODELS_PATH = \"/home/ahmad/adversarial-robustness-for-sr/logs/train/runs\"\n",
    "\n",
    "model_filename = \"sv_baseline-voxceleb-max_dur3.0-bs32\" #\"vpc_amm_cyclic-T10-2-max_dur10-bs32\" \"vpc_norm_enh-available_models-max_dur10-64\", 'vpc_ce-available_models-max_dur10-64', 'vpc_norm_enh-available_models-max_dur10-64', 'vpc_amm-available_models-max_dur10-64', 'vpc_amm-available_models-max_dur10-with_contrastive_loss-64']\n",
    "MODELS_PATH = \"/dataHDD/ahmad/hpc_results/second_run\"\n",
    "\n",
    "model_exp = Path(f'{MODELS_PATH}/{model_filename}')\n",
    "eval_test = True\n",
    "\n",
    "scores_csv_file = 'test_scores.csv' if eval_test else 'valid_best_scores.csv' # 'valid_best_scores.csv' 'best_valid_scores.csv'\n",
    "\n",
    "artifacts_dir = 'voxceleb_artifacts' # 'vpc2025_artifacts' 'voxceleb_artifacts'\n",
    "\n",
    "data_df_file = 'veri_test.csv' #'test.csv' if eval_test else'dev.csv'\n",
    "dirname = 'test' if eval_test else 'valid'\n",
    "\n",
    "tmp = list(model_exp.rglob(f'{dirname}*/{scores_csv_file}'))\n",
    "assert len(tmp) == 1, f'Expected one file called test_scores.csv, found: {len(tmp)}'\n",
    "scores_path = tmp[0]\n",
    "\n",
    "df = pd.read_csv(f'{scores_path}')\n",
    "df_test = pd.read_csv(f'{str(model_exp / f\"{artifacts_dir}/{data_df_file}\")}', sep=\"|\")\n",
    "\n",
    "df['enroll_path'] = df['enroll_path'].apply(lambda x: x.split('voxceleb/voxceleb1_2/')[-1])\n",
    "df['test_path'] = df['test_path'].apply(lambda x: x.split('voxceleb/voxceleb1_2/')[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Trial Scores  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bin_count(data):\n",
    "    \"\"\"Calculate optimal number of bins (Freedman-Diaconis rule)\"\"\"\n",
    "    iqr = np.percentile(data, 75) - np.percentile(data, 25)\n",
    "    bin_width = 2 * iqr * len(data)**(-1/3)  # Freedman-Diaconis rule\n",
    "    return int(np.ceil((data.max() - data.min()) / bin_width)) if bin_width > 0 else 30\n",
    "\n",
    "\n",
    "def plot_score_distribution(df_test, score_schemes=None, figsize=(10, 6), save_dir=None, show_plot=True):\n",
    "    \"\"\"\n",
    "    Plot the distribution of similarity scores for genuine and impostor trials.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df_test : pandas.DataFrame\n",
    "        DataFrame containing similarity scores and labels\n",
    "    score_schemes : list, optional\n",
    "        List of score column names to plot (default: ['score', 'norm_score'] or available columns)\n",
    "    figsize : tuple, optional\n",
    "        Figure size as (width, height) in inches (default: (10, 6))\n",
    "    save_dir : str, optional\n",
    "        Directory path to save plots. If None, plots won't be saved.\n",
    "    show_plot : bool, optional\n",
    "        Whether to display the plot interactively (default: True)\n",
    "    \"\"\"\n",
    "    # Set publication-quality font settings\n",
    "    plt.rcParams.update({\n",
    "        'font.family': 'serif',\n",
    "        'font.serif': ['Times New Roman', 'DejaVu Serif', 'Palatino', 'Computer Modern Roman'],\n",
    "        'font.size': 12,\n",
    "        'axes.labelsize': 14,\n",
    "        'axes.titlesize': 16,\n",
    "        'xtick.labelsize': 12,\n",
    "        'ytick.labelsize': 12,\n",
    "        'legend.fontsize': 12,\n",
    "        'figure.titlesize': 18,\n",
    "        'text.usetex': False,  # Set to True if you have LaTeX installed\n",
    "        'axes.linewidth': 1.2,\n",
    "        'xtick.major.width': 1.2,\n",
    "        'ytick.major.width': 1.2,\n",
    "        'xtick.major.size': 5,\n",
    "        'ytick.major.size': 5\n",
    "    })\n",
    "    \n",
    "    # Set default score schemes if not provided\n",
    "    if score_schemes is None:\n",
    "        default_schemes = ['score', 'norm_score']\n",
    "        # Filter to only keep schemes that exist in the dataframe\n",
    "        score_schemes = [scheme for scheme in default_schemes if scheme in df_test.columns]\n",
    "        \n",
    "        # If none of the default schemes exist, try to find any columns that might be scores\n",
    "        if not score_schemes:\n",
    "            # Look for columns that might contain scores (excluding 'label')\n",
    "            potential_scores = [col for col in df_test.columns if col != 'label']\n",
    "            if potential_scores:\n",
    "                score_schemes = potential_scores[:2]  # Take at most 2 columns\n",
    "                print(f\"Using detected score columns: {score_schemes}\")\n",
    "            else:\n",
    "                raise ValueError(\"No score columns found in the dataframe.\")\n",
    "    else:\n",
    "        # Filter user-provided schemes to only keep those that exist in the dataframe\n",
    "        valid_schemes = [scheme for scheme in score_schemes if scheme in df_test.columns]\n",
    "        if not valid_schemes:\n",
    "            raise ValueError(f\"None of the provided score schemes {score_schemes} exist in the dataframe.\")\n",
    "        \n",
    "        if len(valid_schemes) < len(score_schemes):\n",
    "            print(f\"Warning: Only using valid columns: {valid_schemes}. Skipping non-existent columns.\")\n",
    "        \n",
    "        score_schemes = valid_schemes\n",
    "    \n",
    "    # Create save directory if it doesn't exist\n",
    "    if save_dir and not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    \n",
    "    for score_scheme in score_schemes:\n",
    "        # Set a professional style with whitegrid for clarity\n",
    "        sns.set_style('whitegrid', {\n",
    "            'grid.linestyle': '--',\n",
    "            'grid.alpha': 0.7,\n",
    "            'axes.edgecolor': '0.2',\n",
    "            'axes.grid': True\n",
    "        })\n",
    "        \n",
    "        # Create figure with higher resolution for publication\n",
    "        fig = plt.figure(figsize=figsize, dpi=300)\n",
    "        ax = fig.add_subplot(111)\n",
    "        \n",
    "        # Extract scores by label\n",
    "        neg_scores = df_test[df_test.trial_label == 0][score_scheme]\n",
    "        pos_scores = df_test[df_test.trial_label == 1][score_scheme]\n",
    "        \n",
    "        # Get count of trials\n",
    "        neg_count = len(neg_scores)\n",
    "        pos_count = len(pos_scores)\n",
    "        total_count = neg_count + pos_count\n",
    "        \n",
    "        # Calculate appropriate bin count with upper limit\n",
    "        bins = max(get_bin_count(neg_scores), get_bin_count(pos_scores))\n",
    "#         bins = min(bins, 100)  # Cap at 100 bins for visualization clarity\n",
    "        \n",
    "        # Calculate statistics once\n",
    "        neg_mean = neg_scores.mean()\n",
    "        pos_mean = pos_scores.mean()\n",
    "        separation = pos_mean - neg_mean\n",
    "        \n",
    "        # Create normalized histograms with more professional colors\n",
    "        ax.hist(neg_scores, bins=bins, alpha=0.7, density=True, \n",
    "                color='#D45E5E', edgecolor='#8B0000', linewidth=1.2, \n",
    "                label=f'Impostor Trials (n={neg_count:,})')\n",
    "        ax.hist(pos_scores, bins=bins, alpha=0.7, density=True,\n",
    "                color='#5E81D4', edgecolor='#00008B', linewidth=1.2,\n",
    "                label=f'Genuine Trials (n={pos_count:,})')\n",
    "        \n",
    "        # Add vertical lines for means with publication-quality styling\n",
    "        ax.axvline(neg_mean, color='#8B0000', linestyle='dashed', linewidth=2,\n",
    "                  label=f'Impostor Mean: {neg_mean:.3f}')\n",
    "        ax.axvline(pos_mean, color='#00008B', linestyle='dashed', linewidth=2,\n",
    "                  label=f'Genuine Mean: {pos_mean:.3f}')\n",
    "        \n",
    "        # Add labels and title with improved formatting\n",
    "        score_name = score_scheme.replace('_', ' ').title()\n",
    "        ax.set_xlabel('Similarity Score', fontweight='bold')\n",
    "        ax.set_ylabel('Normalized Frequency', fontweight='bold')\n",
    "        ax.set_title(f'Distribution of Similarity Scores by Trial Type ({score_name})', \n",
    "                    fontweight='bold', pad=15)\n",
    "        \n",
    "        # Customize legend with cleaner appearance\n",
    "        legend = ax.legend(frameon=True, fancybox=False, framealpha=0.95, \n",
    "                          edgecolor='0.2', loc='best')\n",
    "        legend.get_frame().set_linewidth(1.0)\n",
    "        \n",
    "        # Add annotation about the separation and trial counts with cleaner styling\n",
    "        info_text = f\"Separation: {separation:.3f}\\nTotal Trials: {total_count:,}\\nImpostor: {neg_count:,} ({neg_count/total_count:.1%})\\nGenuine: {pos_count:,} ({pos_count/total_count:.1%})\"\n",
    "\n",
    "        ax.annotate(info_text,\n",
    "                   xy=(0.97, 0.97), xycoords='axes fraction', \n",
    "                   bbox=dict(boxstyle=\"round,pad=0.4\", fc=\"#F8F8F8\", ec=\"gray\", \n",
    "                            alpha=0.95, linewidth=1.2),\n",
    "                   ha='right', va='top', fontweight='normal',  # Changed from 'bold' to 'normal'\n",
    "                   fontsize=plt.rcParams['legend.fontsize'])   # Match legend font size\n",
    "        \n",
    "        # Customize spines and ticks for publication quality\n",
    "        for spine in ['top', 'right', 'bottom', 'left']:\n",
    "            ax.spines[spine].set_linewidth(1.2)\n",
    "        ax.tick_params(width=1.2, length=5, direction='out')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save the plot if directory is provided\n",
    "        if save_dir:\n",
    "            # Create filename from score scheme with high resolution\n",
    "            filename = f\"{score_scheme.replace(' ', '_')}_distribution.pdf\"  # PDF for vector quality\n",
    "            filepath = os.path.join(save_dir, filename)\n",
    "            plt.savefig(filepath, dpi=600, bbox_inches='tight', format='pdf')\n",
    "            \n",
    "            # Also save a PNG version for easy viewing\n",
    "            png_filepath = os.path.join(save_dir, f\"{score_scheme.replace(' ', '_')}_distribution.png\")\n",
    "            plt.savefig(png_filepath, dpi=300, bbox_inches='tight')\n",
    "            \n",
    "            print(f\"Saved plot to {filepath} and {png_filepath}\")\n",
    "        \n",
    "        # Show the plot if requested\n",
    "        if show_plot:\n",
    "            plt.show()\n",
    "        else:\n",
    "            plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_analysis_header(model_filename: str):\n",
    "    header = f\"\"\"\n",
    "    <div style=\"background-color: #f0f2f6; padding: 10px; border-radius: 5px; margin: 10px 0;\">\n",
    "        <h3 style=\"color: #2c3e50; margin: 0; text-align: center;\">\n",
    "            Experiment: {model_filename}\n",
    "        </h3>\n",
    "    </div>\n",
    "    \"\"\"\n",
    "    display(HTML(header))\n",
    "\n",
    "def print_analysis_results(metrics, caption=None):\n",
    "    caption_html = f'<p style=\"text-align: center; font-style: italic; margin: 5px 0;\">{caption}</p>' if caption else ''\n",
    "    \n",
    "    results = f\"\"\"\n",
    "    <div style=\"background-color: #f8f9fa; padding: 15px; border-radius: 5px; margin: 10px 0;\">\n",
    "        <h4 style=\"color: #2c3e50; margin: 0 0 10px 0;\">Results Summary:</h4>\n",
    "        {caption_html}\n",
    "        <div style=\"display: flex; justify-content: center; width: 100%;\">\n",
    "            <table style=\"width: 50%; border-collapse: collapse; margin-top: 8px;\">\n",
    "                <tr style=\"background-color: #34495e;\">\n",
    "                    <th style=\"padding: 6px; text-align: center; border: 1px solid #dee2e6; width: 50%; color: white;\"><b>Metric</b></th>\n",
    "                    <th style=\"padding: 6px; text-align: center; border: 1px solid #dee2e6; width: 50%; color: white;\"><b>Value</b></th>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <td style=\"padding: 6px; text-align: center; border: 1px solid #dee2e6;\"><b>EER</b></td>\n",
    "                    <td style=\"padding: 6px; text-align: center; border: 1px solid #dee2e6;\">{metrics['eer']:.3f}</td>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <td style=\"padding: 6px; text-align: center; border: 1px solid #dee2e6;\"><b>MinDCF</b></td>\n",
    "                    <td style=\"padding: 6px; text-align: center; border: 1px solid #dee2e6;\">{metrics['minDCF']:.3f}</td>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <td style=\"padding: 6px; text-align: center; border: 1px solid #dee2e6;\"><b>F_score</b></td>\n",
    "                    <td style=\"padding: 6px; text-align: center; border: 1px solid #dee2e6;\">{metrics['f_score']:.3f}</td>\n",
    "                </tr>\n",
    "            </table>\n",
    "        </div>\n",
    "    </div>\n",
    "    \"\"\"\n",
    "    display(HTML(results))\n",
    "\n",
    "def compute_metrics(df, scores_col='norm_score'):\n",
    "    metric = VerificationMetrics()\n",
    "    metric.reset()\n",
    "    # Compute metrics\n",
    "    metric.update(\n",
    "        torch.from_numpy(df[scores_col].values),\n",
    "        torch.from_numpy(df['trial_label'].values)\n",
    "    )\n",
    "    metrics = metric.compute()\n",
    "    metric.plot_curves()\n",
    "    \n",
    "    return metrics, curves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_speaker_metrics = {}\n",
    "tot_gender_metrics = {}\n",
    "results_dir = f'results/{model_filename}'   # model_exp (maybe)\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "print_analysis_header(model_filename)\n",
    "\n",
    "# Plot scores histogram\n",
    "plot_score_distribution(df, figsize=(12, 8), save_dir=f'{results_dir}/{header}', show_plot=True)\n",
    "\n",
    "scores_cols = ['norm_score', 'score'] if not all(df.score == df.norm_score) else ['score']\n",
    "for scores_col in scores_cols:\n",
    "    metrics, curves = compute_metrics(df, scores_col=scores_col)\n",
    "    caption = 'RAW Scores' if scores_col == 'score' else 'Normalized Scores'\n",
    "    print_analysis_results(metrics, caption='Table: ' + caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
