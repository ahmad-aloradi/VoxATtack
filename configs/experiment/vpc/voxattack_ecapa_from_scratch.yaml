# @package _global_

# to execute this experiment run:
# python train.py experiment=example

defaults:
  - override /callbacks: default.yaml
  - override /datamodule: datasets/vpc.yaml
  - override /module: vpc.yaml
  - override /trainer: gpu.yaml
  - override /logger: tensorboard.yaml

# all parameters below will be merged with parameters from default configurations set above
# this allows you to overwrite only specified parameters

tags: ["vpc25", "de-anonmyzation", "amm", "comprehensive"]
seed: 42

trainer:
  min_epochs: 10
  max_epochs: 25
  gradient_clip_val: 1.0
  num_sanity_val_steps: 0
  check_val_every_n_epoch: 1

callbacks:
  model_checkpoint:
    mode: min
    monitor: ${replace:"valid/__metric__"}/${module.metrics.valid_best.target_key}
  early_stopping:
    mode: min
    monitor: ${replace:"valid/__metric__"}/${module.metrics.valid_best.target_key}
    patience: 10

datamodule:
  dataset:
    max_duration: 10.0

  models: null

  loaders:
    train:
      batch_size: 32
    valid:
      batch_size: 32
    test:
      batch_size: 4
    enrollment:
      batch_size: 1

module:
  model:
    audio_processor:
      _target_: speechbrain.lobes.features.Fbank
      sample_rate: ${datamodule.dataset.sample_rate}
      deltas: False
      n_mels: 80
      f_min: 0
      f_max: null
      n_fft: 512
      win_length: 25
      hop_length: 10
      left_frames: 0
      right_frames: 0

    audio_processor_normalizer:
      _target_: speechbrain.processing.features.InputNormalization
      norm_type: sentence
      std_norm: False

    audio_encoder:
      _target_: speechbrain.lobes.models.ECAPA_TDNN.ECAPA_TDNN
      input_size: ${module.model.audio_processor.n_mels}
      channels: [1024, 1024, 1024, 1024, 3072]
      kernel_sizes: [5, 3, 3, 3, 1]
      dilations: [1, 2, 3, 4, 1]
      groups: [1, 1, 1, 1, 1]
      attention_channels: 128
      lin_neurons: 192  # ECAPA embedding size

  normalize_test_scores: True
      
  criterion:
    train_criterion:
      _target_: src.modules.losses.components.multi_modal_losses.MultiModalLoss
      classifier_name: robust
      weights:
        _target_: src.modules.losses.components.multi_modal_losses.LossWeights
        ensemble: 1.0
        fusion: 0.1
        audio: 0.1
        text: 0.1
        contrastive: 0.0
        consistency: 0.0
        confidence: 0.0
      confidence_target: 0.9
      classification_loss: 
        _target_: speechbrain.nnet.losses.LogSoftmaxWrapper
        loss_fn:
          _target_: speechbrain.nnet.losses.AdditiveAngularMargin
          margin: 0.15
          scale: 30.0

  lr_scheduler: 
    scheduler:
        _target_: torch.optim.lr_scheduler.CyclicLR
        base_lr: 1e-6
        max_lr: ${module.optimizer.lr}
        step_size_up: ${oc.eval:"int(2 * 104015 / ${datamodule.loaders.train.batch_size})"} # Full cycle every 4 epochs
    extras:
      monitor: ${replace:"valid/__metric__"}
      interval: step
      frequency: 1


  optimizer:
    lr: 1.0e-4
    weight_decay: 1.0e-5


# logger:
#   wandb:
#     tags: ${tags}
#     group: "mnist"
