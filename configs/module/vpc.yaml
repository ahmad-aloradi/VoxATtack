_target_: src.modules.multimodal_vpc.MultiModalVPCModel

defaults:
  - _self_
  - model: robust.yaml  # Reference to a config group

model:
  audio_processor:
    _target_: torch.nn.Identity

  audio_encoder:
    _target_: speechbrain.inference.EncoderClassifier.from_hparams
    source: "speechbrain/spkrec-ecapa-voxceleb"
    savedir: "local/.pretrained_models/spkrec-ecapa-voxceleb"
    run_opts:
      device: "cuda"

  audio_processor_kwargs:
    return_tensors: "pt"
    sampling_rate: 16000

  text_processor:
    _target_: transformers.BertTokenizer.from_pretrained
    pretrained_model_name_or_path: "bert-base-uncased"

  text_encoder:
    _target_: transformers.BertModel.from_pretrained
    pretrained_model_name_or_path: ${module.model.text_processor.pretrained_model_name_or_path}
    num_labels: ${datamodule.num_classes}

  text_processor_kwargs:
    return_tensors: "pt"
    padding: "max_length"
    truncation: True
    max_length: 512

### classifier ###
  # fusion_classifier: ${module.fusion_classifier}

### caching ###
  embedding_cache:
    max_size: 500000
    bypass_warmup: True

### criteria and metrics ###
criterion:
  train_criterion: ???
    
  # For callbacks
  loss: ${module.criterion.train_criterion}

metrics:
  train:
    _target_: "torchmetrics.Accuracy"
    task: "multiclass"
    num_classes: ${datamodule.num_classes}
  valid:
    _target_: src.modules.metrics.metrics.VerificationMetrics
  test:
    _target_: src.modules.metrics.metrics.VerificationMetrics
  valid_best:
    _target_: src.modules.metrics.metrics.AutoSyncDictMinMetric
    target_key: "eer"

### optimizer and lr_scheduler ###

optimizer:
  _target_: torch.optim.AdamW
  _partial_: True
  lr: 1.0e-4
  weight_decay: 1.0e-5

lr_scheduler: ???

### extras and callbacks ###
logging_params:
  on_step: False
  on_epoch: True
  sync_dist: True
  prog_bar: True


### score normalization ###
normalize_test_scores: ???   # apply score normalization or not
scores_norm:
  embeds_metric_params:
    cohort_per_model: 40000
  scores_norm_params:
    topk: 1000
    min_cohort_size: 3000
