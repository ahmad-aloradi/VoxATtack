# Robust Speaker Recognition Against Adversarial Attacks and Spoofing

<a href="https://pytorch.org/get-started/locally/"><img alt="PyTorch" src="https://img.shields.io/badge/PyTorch-ee4c2c?logo=pytorch&logoColor=white"></a>
<a href="https://pytorchlightning.ai/"><img alt="Lightning" src="https://img.shields.io/badge/-Lightning-792ee5?logo=pytorchlightning&logoColor=white"></a>
<a href="https://hydra.cc/"><img alt="Config: Hydra" src="https://img.shields.io/badge/Config-Hydra-89b8cd"></a>
<a href="https://github.com/gorodnitskiy/yet-another-lightning-hydra-template"><img alt="Template" src="https://img.shields.io/badge/-Lightning--Hydra--Template-017F2F?style=flat&logo=github&labelColor=gray"></a>
[![Publication](https://img.shields.io/badge/Paper-In%20progress-red)]()<br>

## Description

This is the official implementation for the paper: Aloradi et al. "VoxATack a MultiModal Attack on Voice Anonymization Systems", WASPAA Lake Tahoe, 2025. 

The framework is based on [this template](https://github.com/gorodnitskiy/yet-another-lightning-hydra-template), which is based on
[PyTorch Lightning](https://github.com/Lightning-AI/lightning) and [Hydra](https://github.com/facebookresearch/hydra). 


## Get started

```shell
# clone template
git clone https://github.com/ahmad-aloradi/VoxATtack.git
cd VoxATtack

# install requirements
pip install -r requirements.txt
```

### DATASETS

We are using the `VoicePrivacy2025` and `Librispeech` datasets. Please download them before hand to use the repo. Once downloaded, we recommend you create a symlink in data

```shell
ln -s YOUR_DATA_PATH data/.
```


## Project structure

- `src/`
- `data/`
- `logs/`
- `tests/`
- some additional directories, like: `notebooks/`, `docs/`, etc.

In this particular case, the directory structure looks like:

```
├── configs                     <- Hydra configuration files
│   ├── callbacks               <- Callbacks configs
│   ├── datamodule              <- Datamodule configs
│   ├── debug                   <- Debugging configs
│   ├── experiment              <- Experiment configs
│   ├── extras                  <- Extra utilities configs
│   ├── hparams_search          <- Hyperparameter search configs
│   ├── hydra                   <- Hydra settings configs
│   ├── local                   <- Local configs
│   ├── logger                  <- Logger configs
│   ├── module                  <- Module configs
│   ├── paths                   <- Project paths configs
│   ├── trainer                 <- Trainer configs
│   │
│   ├── eval.yaml               <- Main config for evaluation
│   └── train.yaml              <- Main config for training
│
├── data                        <- Project data
├── logs                        <- Logs generated by hydra, lightning loggers, etc.
├── notebooks                   <- Jupyter notebooks.
├── scripts                     <- Shell scripts
│
├── src                         <- Source code
│   ├── callbacks               <- Additional callbacks
│   ├── datamodules             <- Lightning datamodules
│   ├── modules                 <- Lightning modules
│   ├── utils                   <- Utility scripts
│   │
│   ├── eval.py                 <- Run evaluation
│   └── train.py                <- Run training
│
├── tests                       <- Tests of any kind
│
├── .dockerignore               <- List of files ignored by docker
├── .gitattributes              <- List of additional attributes to pathnames
├── .gitignore                  <- List of files ignored by git
├── .pre-commit-config.yaml     <- Configuration of pre-commit hooks for code formatting
├── Dockerfile                  <- Dockerfile
├── Makefile                    <- Makefile with commands like `make train` or `make test`
├── pyproject.toml              <- Configuration options for testing and linting
├── requirements.txt            <- File for installing python dependencies
├── setup.py                    <- File for installing project as a package
└── README.md
```

## Data Preparation
### Structure
Our pipeline collect data as `.csv` files with a certain columns, which are defined in `src/datamodules/components/common.py` as:
```python
@dataclass(frozen=True)
class BaseDatasetCols:
    DATASET: Literal['dataset_name'] = 'dataset_name'
    LANGUAGE: Literal['language'] = 'language'
    NATIONALITY: Literal['country'] = 'country'
    SR: Literal['sample_rate'] = 'sample_rate'
    SPEAKER_ID: Literal['speaker_id'] = 'speaker_id'
    CLASS_ID: Literal['class_id'] = 'class_id'
    SPEAKER_NAME: Literal['speaker_name'] = 'speaker_name'
    GENDER: Literal['gender'] = 'gender'
    SPLIT: Literal['split'] = 'split'
    REC_DURATION: Literal['recording_duration'] = 'recording_duration'
    REL_FILEPATH: Literal['rel_filepath'] = 'rel_filepath'
    TEXT: Literal['text'] = 'text'
```

### Preprare the csvs
Follow `scripts/datasets/prep_vpc.sh`. If you face any problems with these scripts, please report to ahmad.aloradi94@gmail.com.

#### Known Issues: 
1. `VoicePrivacy2025` dataset: when untarring the `T25-1` model's data, there is a mis-named folder. Please fix the typo manually.
2. `LibriSpeech` dataset: In `SPEAKERS.TXT`, line 60 used to create a problem when loading as `.csv` with `sep='|'`. It is now automatically handleded.

## Results
Our results can be found [In this link](https://faubox.rrze.uni-erlangen.de/getlink/fi9RnHmqs8AUdfp5tiNsQX/proposed_results%20Table2)

## Usage

### Training

The framework uses Hydra for configuration management. You can train models using different experiment configurations:

#### Basic Training
```bash
# Train with default configuration
python src/train.py

# Train with a specific experiment configuration
python src/train.py experiment=vpc/vpc_amm_cyclic

# Train with custom parameters
python src/train.py trainer.max_epochs=50 datamodule.loaders.train.batch_size=32
```

#### Available VPC Experiments (Recommended)

1. **VoxATtack (multimodal attack)**

   ```shell
   python src/train.py experiment=vpc/voxattack_base
   ```
   - Uses multimodal approach with audio and text features
   - Includes ensemble, fusion, audio, and text loss components

2. **VoxATtack With Data Augmentation (SpeechAugment)**
   ```bash
   python src/train.py experiment=vpc/voxattack_aug
   ```
   - Includes noise addition, reverberation, frequency dropping, chunk dropping, and speed perturbation
   - Automatically downloads and prepares noise and RIR datasets

3. **ECAPA_ours (Audio-Only Model)**
   ```bash
   python src/train.py experiment=vpc/ecapa_ours_base
   ```
   - Audio-only approach using robust audio model
   - Using a single AAM loss term

4. **ECAPA_ours with Data Augmentation (SpeechAugment)**
   ```bash
   python src/train.py experiment=vpc/ecapa_ours_aug
   ```
   - Audio-only approach using robust audio model
   - Simplified loss function without multimodal components

5. **VoxATtack with non-pretrained ECAPA (not shown in the paper)**
   ```bash
   python src/train.py experiment=vpc/voxattack_ecapa_from_scratch
   ```
   - Trains ECAPA-TDNN encoder from scratch

### Evaluation

#### Basic evaluation
```bash
# Evaluate a trained model
python src/eval.py ckpt_path=/path/to/checkpoint.ckpt
```

#### Detailed evaluation (Recommended)
Consider running `notebooks/test_results_analysis.ipynb`. You simply need to adapt the following variables:
```shell
MODELS_PATH = PATH_TO_YOUR_MODELS # Update this to your experiments directory
EVAL_MODE = "EVAL_ALL"  # Options: "SINGLE", "EVAL_ALL". Eval a single experiment or all experiment (in MODELS_PATH)
SINGLE_EXPERIMENT = #your experiment re patterns (if SINGLE)
EXPERIMENT_PATTERN = #select all experiments that matches an re patterns (if EVAL_ALL)
EVAL_TEST = False  # Whether to evaluate test or validation data
```


### Configuration Override Examples

You can override any configuration parameter from the command line:

```bash
# Change learning rate and batch size
python src/train.py module.optimizer.lr=1e-3 datamodule.loaders.train.batch_size=64

# Use different trainer settings
python src/train.py trainer=cpu trainer.max_epochs=10

# Change logger
python src/train.py logger=wandb

# Disable testing after training
python src/train.py test=false

# Use different random seed
python src/train.py seed=1234
```

### Data Augmentation Configuration

When using the augmentation experiment (`*_aug`), the model will automatically:

1. Download noise dataset from Dropbox if not present
2. Download RIR (Room Impulse Response) dataset if not present
3. Prepare CSV annotations for both datasets
4. Apply the following augmentations during training:
   - **Noise Addition**: SNR between 0-15 dB
   - **Reverberation**: Using room impulse responses
   - **Frequency Dropping**: Random frequency band removal
   - **Chunk Dropping**: Random time segment removal
   - **Speed Perturbation**: 90% and 110% speed variations

### Logging and Monitoring

The framework supports multiple logging backends:

```bash
# Use TensorBoard (default)
python src/train.py logger=tensorboard

# Use Weights & Biases
python src/train.py logger=wandb

# Use multiple loggers
python src/train.py logger=many_loggers

# Disable logging
python src/train.py logger=null
```

### GPU/CPU Training

```bash
# Train on GPU (default)
python src/train.py trainer=gpu

# Train on CPU
python src/train.py trainer=cpu

# Multi-GPU training
python src/train.py trainer=ddp
```

## Troubleshooting
Use debug configurations if needed

```bash
# Fast development run with limited data
python src/train.py debug=default

# Overfit on small batch for debugging
python src/train.py debug=overfit

# Profile the training process
python src/train.py debug=profiler
```
